"""
DNA-GPT ë°°ì¹˜ íƒì§€ ìŠ¤í¬ë¦½íŠ¸
===========================
sample_001.jsonlì˜ abstract_hwt, abstract_onlyllm, abstract_rag ê° í•„ë“œ í…ìŠ¤íŠ¸ë¥¼
DNA-GPT ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ íƒì§€í•˜ì—¬ AI/Human íŒì • ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

ì‚¬ìš©ë²•:
    # OpenAI API ëª¨ë“œ
    python run_detection.py --input datasets/sample_001.jsonl --api-key YOUR_KEY
    python run_detection.py --input datasets/sample_001.jsonl --api-key YOUR_KEY --limit 5

    # ë¡œì»¬ ëª¨ë¸ ëª¨ë“œ (LLaMA 3.1 8B ë“±)
    python run_detection.py --input datasets/sample_001.jsonl --local meta-llama/Llama-3.1-8B-Instruct
    python run_detection.py --input datasets/sample_001.jsonl --local meta-llama/Llama-3.1-8B-Instruct --limit 5
    python run_detection.py --input datasets/sample_001.jsonl --dry-run
"""

import argparse
import json
import os
import sys
import time
import re
from datetime import datetime

import ssl
import nltk
import numpy as np

# â”€â”€ NLTK ì„¤ì • â”€â”€
ssl._create_default_https_context = ssl._create_unverified_context
nltk.download('punkt', quiet=True)
nltk.download('punkt_tab', quiet=True)

# â”€â”€ rouge_scoreì—ì„œ _create_ngrams ê°€ì ¸ì˜¤ê¸° â”€â”€
import six
from nltk.stem.porter import PorterStemmer
from rouge_score.rouge_scorer import _create_ngrams

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  DNA-GPT í•µì‹¬ í•¨ìˆ˜ë“¤ (ì›ë³¸ì—ì„œ ì¶”ì¶œ + ë²„ê·¸ ìˆ˜ì •)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_stemmer = PorterStemmer()

try:
    import spacy
    _nlp = spacy.load('en_core_web_sm')
    _stopwords = _nlp.Defaults.stop_words
except Exception:
    _stopwords = set()


def tokenize(text, stemmer=_stemmer, stopwords=_stopwords):
    """í…ìŠ¤íŠ¸ë¥¼ í† í° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì†Œë¬¸ìí™” + ìŠ¤í…Œë° + ë¶ˆìš©ì–´ ì œê±°)"""
    text = text.lower()
    text = re.sub(r"[^a-z0-9]+", " ", six.ensure_str(text))
    tokens = re.split(r"\s+", text)
    if stemmer:
        tokens = [stemmer.stem(x) if len(x) > 3 else x
                  for x in tokens if x not in stopwords]
    tokens = [x for x in tokens if re.match(r"^[a-z0-9]+$", six.ensure_str(x))]
    return tokens


def get_score_ngrams(target_ngrams, prediction_ngrams):
    """ë‘ N-gram ì§‘í•©ì˜ ê²¹ì¹¨ ë¹„ìœ¨ ê³„ì‚°"""
    intersection_count = 0
    ngram_dict = {}
    for ngram in six.iterkeys(target_ngrams):
        intersection_count += min(target_ngrams[ngram], prediction_ngrams[ngram])
        ngram_dict[ngram] = min(target_ngrams[ngram], prediction_ngrams[ngram])
    target_count = sum(target_ngrams.values())
    return intersection_count / max(target_count, 1), ngram_dict


def get_ngram_info(article_tokens, summary_tokens, _ngram):
    """ë‘ ë¬¸ì„œì˜ N-gram ê²¹ì¹¨ ì ìˆ˜ ê³„ì‚°"""
    article_ngram = _create_ngrams(article_tokens, _ngram)
    summary_ngram = _create_ngrams(summary_tokens, _ngram)
    ngram_score, ngram_dict = get_score_ngrams(article_ngram, summary_ngram)
    return ngram_score, ngram_dict, sum(ngram_dict.values())


def N_gram_detector(ngram_n_ratio):
    """N=3~25 N-gram ê²¹ì¹¨ ì ìˆ˜ë¥¼ ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ê³„ì‚° (decay weighting: n*log(n))"""
    score = 0
    non_zero = []
    for idx, key in enumerate(ngram_n_ratio):
        if idx in range(3) and ('score' in key or 'ratio' in key):
            score += 0.0 * ngram_n_ratio[key]
            continue
        if 'score' in key or 'ratio' in key:
            score += (idx + 1) * np.log(idx + 1) * ngram_n_ratio[key]
            if ngram_n_ratio[key] != 0:
                non_zero.append(idx + 1)
    return score / (sum(non_zero) + 1e-8)


def truncate_string_by_words(string, max_words):
    """ë‹¨ì–´ ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ ìë¥´ê¸°"""
    words = string.split()
    if len(words) <= max_words:
        return string
    return ' '.join(words[:max_words])


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  ë¡œì»¬ ëª¨ë¸ ë¡œë“œ + ì¬ìƒì„±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_local_model(model_path, device_map="auto"):
    """ë¡œì»¬ HuggingFace ëª¨ë¸ì„ FP16ìœ¼ë¡œ ë¡œë“œ.
    device_map='auto'ë¡œ VRAM ë¶€ì¡± ì‹œ ìë™ìœ¼ë¡œ RAMì— ë¶„ë°°."""
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer

    print(f"ëª¨ë¸ ë¡œë”©: {model_path} (FP16, device_map={device_map})")
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map=device_map,
    )
    model.eval()

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # VRAM ì‚¬ìš©ëŸ‰ ì¶œë ¥
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        print(f"GPU ë©”ëª¨ë¦¬: {allocated:.1f}GB ì‚¬ìš© / {reserved:.1f}GB ì˜ˆì•½")

    print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    return model, tokenizer


def local_generate(prefix, model, tokenizer, max_new_tokens=300,
                   temperature=0.7, num_return=1):
    """ë¡œì»¬ ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ ì¬ìƒì„±."""
    import torch

    # Instruct ëª¨ë¸ì´ë©´ chat template ì‚¬ìš©
    if hasattr(tokenizer, 'chat_template') and tokenizer.chat_template:
        messages = [
            {"role": "system",
             "content": "You are a helpful assistant that continues the passage from the sentences provided."},
            {"role": "user",
             "content": f"Continue the following text in around 300 words:\n\n{prefix}"},
        ]
        prompt_text = tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True)
    else:
        prompt_text = prefix

    inputs = tokenizer(prompt_text, return_tensors="pt", truncation=True, max_length=2048)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    results = []
    for _ in range(num_return):
        with torch.no_grad():
            output = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                do_sample=True,
                top_p=0.9,
                pad_token_id=tokenizer.pad_token_id,
            )
        # ì…ë ¥ ë¶€ë¶„ ì œê±°í•˜ê³  ìƒì„±ëœ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        gen_ids = output[0][inputs['input_ids'].shape[1]:]
        gen_text = tokenizer.decode(gen_ids, skip_special_tokens=True)
        results.append(gen_text)

    return results


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  OpenAI / ë¡œì»¬ ëª¨ë¸ íƒì§€ (ë²„ê·¸ ìˆ˜ì • ë²„ì „)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def detect_text(text, client, model_name="gpt-3.5-turbo",
                truncate_ratio=0.5, threshold=0.00025,
                regen_number=10, max_new_tokens=300,
                temperature=0.7, max_words=350,
                local_model=None, local_tokenizer=None):
    """
    DNA-GPT íƒì§€: í…ìŠ¤íŠ¸ë¥¼ ë°˜ìœ¼ë¡œ ì˜ë¼ ë’·ë¶€ë¶„ì„ ì¬ìƒì„±í•œ ë’¤
    ì›ë³¸ suffixì™€ ì¬ìƒì„±ë³¸ì˜ N-gram ê²¹ì¹¨ ì ìˆ˜ë¡œ AI ì—¬ë¶€ë¥¼ íŒì •.
    local_modelì´ ì£¼ì–´ì§€ë©´ ë¡œì»¬ ëª¨ë¸ë¡œ, ì•„ë‹ˆë©´ OpenAI APIë¡œ ì¬ìƒì„±.

    Returns:
        dict: {
            'decision': bool (True=AI, False=Human),
            'score': float (N-gram overlap score),
            'threshold': float,
            'regen_count': int (ì‹¤ì œ ì¬ìƒì„± ìˆ˜),
        }
    """
    text = truncate_string_by_words(text, max_words)

    if len(text.strip()) < 50:
        return {
            'decision': None,
            'score': 0.0,
            'threshold': threshold,
            'regen_count': 0,
            'error': 'text_too_short'
        }

    # í…ìŠ¤íŠ¸ë¥¼ prefix / suffixë¡œ ë¶„ë¦¬
    split_point = int(truncate_ratio * len(text))
    prefix = text[:split_point]
    suffix = text[split_point:]
    suffix_tokens = tokenize(suffix)

    if len(suffix_tokens) == 0:
        return {
            'decision': None,
            'score': 0.0,
            'threshold': threshold,
            'regen_count': 0,
            'error': 'empty_suffix_tokens'
        }

    # â”€â”€ ì¬ìƒì„± â”€â”€
    regen_texts = []
    try:
        if local_model is not None and local_tokenizer is not None:
            # â”€â”€ ë¡œì»¬ ëª¨ë¸ë¡œ ì¬ìƒì„± â”€â”€
            regen_texts = local_generate(
                prefix, local_model, local_tokenizer,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                num_return=regen_number,
            )
        elif model_name in ("gpt-3.5-turbo-instruct",):
            # Completions API (instruct ëª¨ë¸)
            response = client.completions.create(
                model=model_name,
                prompt=prefix,
                max_tokens=max_new_tokens,
                temperature=temperature,
                n=regen_number
            )
            regen_texts = [c.text for c in response.choices]
        else:
            # Chat Completions API (gpt-3.5-turbo, gpt-4 ë“±)
            for _ in range(regen_number):
                response = client.chat.completions.create(
                    model=model_name,
                    messages=[
                        {"role": "system",
                         "content": "You are a helpful assistant that continues the passage from the sentences provided."},
                        {"role": "user",
                         "content": "continues the passage from the current text within in total around 300 words:"},
                        {"role": "assistant",
                         "content": prefix},
                    ],
                    temperature=temperature,
                    max_tokens=max_new_tokens,
                )
                regen_texts.append(response.choices[0].message.content)
    except Exception as e:
        return {
            'decision': None,
            'score': 0.0,
            'threshold': threshold,
            'regen_count': 0,
            'error': str(e)
        }

    # â”€â”€ N-gram ê²¹ì¹¨ ì ìˆ˜ ê³„ì‚° â”€â”€
    gpt_scores = []
    for gen_text in regen_texts:
        gen_text_truncated = truncate_string_by_words(gen_text, max_words - 150)
        gen_tokens = tokenize(gen_text_truncated)
        if len(gen_tokens) == 0:
            continue

        temp1 = {}
        for _ngram in range(1, 25):
            ngram_score, ngram_dict, overlap_count = get_ngram_info(
                suffix_tokens, gen_tokens, _ngram)
            temp1[f'ngram_{_ngram}_score'] = ngram_score / len(gen_tokens)
            temp1[f'ngram_{_ngram}_ngramdict'] = ngram_dict
            temp1[f'ngram_{_ngram}_count'] = overlap_count

        score = N_gram_detector(temp1)
        gpt_scores.append(score)

    if len(gpt_scores) == 0:
        return {
            'decision': None,
            'score': 0.0,
            'threshold': threshold,
            'regen_count': 0,
            'error': 'no_valid_regenerations'
        }

    avg_score = float(np.mean(gpt_scores))

    return {
        'decision': avg_score > threshold,
        'score': avg_score,
        'threshold': threshold,
        'regen_count': len(gpt_scores),
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  ë°ì´í„° ë¡œë“œ + ë°°ì¹˜ ì‹¤í–‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_data(path):
    """sample_001.jsonl ë¡œë“œ"""
    records = []
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                records.append(json.loads(line))
    return records


def run_batch(records, client, args):
    """
    ê° ë ˆì½”ë“œì˜ abstract_hwt, abstract_onlyllm, abstract_ragë¥¼
    DNA-GPTë¡œ íƒì§€í•˜ê³  ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•œë‹¤.
    """
    results = []
    total = len(records)

    fields = [
        ('abstract_hwt', 'human'),        # ê¸°ëŒ€: Human (decision=False)
        ('abstract_only_llm', 'ai'),       # ê¸°ëŒ€: AI (decision=True)
        ('abstract_rag', 'ai'),            # ê¸°ëŒ€: AI (decision=True)
    ]

    for i, record in enumerate(records):
        paper_id = record.get('paper_id', f'unknown_{i}')
        print(f"\n{'='*60}")
        print(f"[{i+1}/{total}] Paper: {paper_id}")
        print(f"{'='*60}")

        record_result = {
            'paper_id': paper_id,
            'keyword': record.get('keyword', ''),
        }

        for field_name, expected_label in fields:
            text = record.get(field_name, '')
            if not text or not text.strip():
                print(f"  {field_name}: (ë¹ˆ í…ìŠ¤íŠ¸ - ê±´ë„ˆëœ€)")
                record_result[field_name] = {
                    'decision': None,
                    'score': 0.0,
                    'expected': expected_label,
                    'correct': None,
                    'error': 'empty_text'
                }
                continue

            word_count = len(text.split())
            print(f"  {field_name} ({word_count} words, expected={expected_label})...", end=" ", flush=True)

            if args.dry_run:
                # dry-run ëª¨ë“œ: API í˜¸ì¶œ ì—†ì´ ë°ì´í„° êµ¬ì¡°ë§Œ í™•ì¸
                result = {
                    'decision': None,
                    'score': 0.0,
                    'threshold': args.threshold,
                    'regen_count': 0,
                    'dry_run': True
                }
            else:
                result = detect_text(
                    text=text,
                    client=client,
                    model_name=args.model_name,
                    truncate_ratio=args.truncate_ratio,
                    threshold=args.threshold,
                    regen_number=args.regen_number,
                    max_new_tokens=args.max_tokens,
                    temperature=args.temperature,
                    max_words=args.max_words,
                    local_model=getattr(args, '_local_model', None),
                    local_tokenizer=getattr(args, '_local_tokenizer', None),
                )

            result['expected'] = expected_label
            if result['decision'] is not None:
                predicted = 'ai' if result['decision'] else 'human'
                result['predicted'] = predicted
                result['correct'] = (predicted == expected_label)
                decision_str = "ğŸ¤– AI" if result['decision'] else "ğŸ‘¤ Human"
                correct_str = "âœ…" if result['correct'] else "âŒ"
                print(f"{decision_str} (score={result['score']:.6f}) {correct_str}")
            else:
                result['predicted'] = None
                result['correct'] = None
                error = result.get('error', result.get('dry_run', ''))
                print(f"âš ï¸ íŒì •ë¶ˆê°€ ({error})")

            record_result[field_name] = result

            # API rate limit ëŒ€ë¹„ ë”œë ˆì´
            if not args.dry_run:
                time.sleep(args.delay)

        results.append(record_result)

    return results


def print_summary(results):
    """íƒì§€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    fields = ['abstract_hwt', 'abstract_only_llm', 'abstract_rag']
    field_labels = {
        'abstract_hwt': 'Human (hwt)',
        'abstract_only_llm': 'AI (only_llm)',
        'abstract_rag': 'AI (rag)',
    }

    print(f"\n{'='*60}")
    print("íƒì§€ ê²°ê³¼ ìš”ì•½")
    print(f"{'='*60}")

    total_correct = 0
    total_tested = 0

    for field in fields:
        correct = 0
        tested = 0
        scores = []
        for r in results:
            fr = r.get(field, {})
            if fr.get('correct') is not None:
                tested += 1
                if fr['correct']:
                    correct += 1
                scores.append(fr['score'])

        total_correct += correct
        total_tested += tested

        if tested > 0:
            acc = correct / tested * 100
            avg_score = np.mean(scores) if scores else 0
            print(f"  {field_labels[field]:20s}: {correct}/{tested} ì •í™• ({acc:.1f}%), "
                  f"í‰ê·  score={avg_score:.6f}")
        else:
            print(f"  {field_labels[field]:20s}: í…ŒìŠ¤íŠ¸ ì—†ìŒ")

    if total_tested > 0:
        overall_acc = total_correct / total_tested * 100
        print(f"\n  {'ì „ì²´':20s}: {total_correct}/{total_tested} ì •í™• ({overall_acc:.1f}%)")


def save_results(results, output_path):
    """ê²°ê³¼ë¥¼ JSONL íŒŒì¼ë¡œ ì €ì¥"""
    with open(output_path, 'w', encoding='utf-8') as f:
        for r in results:
            # ngram dictëŠ” ì§ë ¬í™” ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ ì œê±°
            clean_r = {}
            for k, v in r.items():
                if isinstance(v, dict):
                    clean_v = {kk: vv for kk, vv in v.items()
                               if not isinstance(vv, dict) or kk in ('error',)}
                    clean_r[k] = clean_v
                else:
                    clean_r[k] = v
            f.write(json.dumps(clean_r, ensure_ascii=False) + '\n')
    print(f"\nê²°ê³¼ ì €ì¥: {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description="DNA-GPT ë°°ì¹˜ íƒì§€: sample_001.jsonlì˜ ì„¸ í•„ë“œë¥¼ AI/Humanìœ¼ë¡œ íŒì •",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # dry-run (API í˜¸ì¶œ ì—†ì´ ë°ì´í„° êµ¬ì¡° í™•ì¸)
  python run_detection.py --input datasets/sample_001.jsonl --dry-run

  # ë¡œì»¬ ëª¨ë¸ (LLaMA 3.1 8B Instruct, FP16)
  python run_detection.py -i datasets/sample_001.jsonl --local meta-llama/Llama-3.1-8B-Instruct --limit 3

  # ë¡œì»¬ ëª¨ë¸ + ì¬ìƒì„± ì¤„ì´ê¸° (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
  python run_detection.py -i datasets/sample_001.jsonl --local meta-llama/Llama-3.1-8B-Instruct --regen 3 --limit 5

  # OpenAI API ëª¨ë“œ
  python run_detection.py -i datasets/sample_001.jsonl --api-key sk-... --limit 5
  python run_detection.py -i datasets/sample_001.jsonl --api-key sk-... --model gpt-4
        """
    )

    parser.add_argument('--input', '-i', required=True,
                        help='sample_001.jsonl ê²½ë¡œ')
    parser.add_argument('--local', default=None, metavar='MODEL_PATH',
                        help='ë¡œì»¬ HuggingFace ëª¨ë¸ ê²½ë¡œ ë˜ëŠ” ID (ì˜ˆ: meta-llama/Llama-3.1-8B-Instruct)')
    parser.add_argument('--api-key', '-k', default=None,
                        help='OpenAI API key (ë˜ëŠ” OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜)')
    parser.add_argument('--model', dest='model_name', default='gpt-3.5-turbo',
                        help='ì¬ìƒì„±ì— ì‚¬ìš©í•  OpenAI ëª¨ë¸ (ê¸°ë³¸: gpt-3.5-turbo)')
    parser.add_argument('--limit', '-n', type=int, default=None,
                        help='í…ŒìŠ¤íŠ¸í•  ë ˆì½”ë“œ ìˆ˜ ì œí•œ')
    parser.add_argument('--regen', dest='regen_number', type=int, default=10,
                        help='ì¬ìƒì„± íšŸìˆ˜ (ê¸°ë³¸: 10, ì›ë³¸: 30)')
    parser.add_argument('--threshold', type=float, default=0.00025,
                        help='AI íŒì • ì„ê³„ê°’ (ê¸°ë³¸: 0.00025)')
    parser.add_argument('--truncate-ratio', type=float, default=0.5,
                        help='í…ìŠ¤íŠ¸ ë¶„í•  ë¹„ìœ¨ (ê¸°ë³¸: 0.5)')
    parser.add_argument('--max-tokens', type=int, default=300,
                        help='ì¬ìƒì„± ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸: 300)')
    parser.add_argument('--max-words', type=int, default=350,
                        help='ì…ë ¥ í…ìŠ¤íŠ¸ ìµœëŒ€ ë‹¨ì–´ ìˆ˜ (ê¸°ë³¸: 350)')
    parser.add_argument('--temperature', type=float, default=0.7,
                        help='ì¬ìƒì„± ì˜¨ë„ (ê¸°ë³¸: 0.7)')
    parser.add_argument('--delay', type=float, default=1.0,
                        help='API í˜¸ì¶œ ê°„ ë”œë ˆì´(ì´ˆ) (ê¸°ë³¸: 1.0)')
    parser.add_argument('--output', '-o', default=None,
                        help='ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ê¸°ë³¸: results/detection_ê²°ê³¼_íƒ€ì„ìŠ¤íƒ¬í”„.jsonl)')
    parser.add_argument('--dry-run', action='store_true',
                        help='API í˜¸ì¶œ ì—†ì´ ë°ì´í„° êµ¬ì¡°ë§Œ í™•ì¸')

    args = parser.parse_args()

    # â”€â”€ ëª¨ë“œ ê²°ì • â”€â”€
    use_local = args.local is not None

    if not use_local:
        api_key = args.api_key or os.environ.get('OPENAI_API_KEY', '')
        if not api_key and not args.dry_run:
            print("âŒ OpenAI API keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            print("   --api-key ì˜µì…˜ ë˜ëŠ” OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
            print("   ë˜ëŠ” --local ì˜µì…˜ìœ¼ë¡œ ë¡œì»¬ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            sys.exit(1)

    # â”€â”€ ë°ì´í„° ë¡œë“œ â”€â”€
    print(f"ë°ì´í„° ë¡œë“œ: {args.input}")
    records = load_data(args.input)
    print(f"ì „ì²´ ë ˆì½”ë“œ: {len(records)}ê±´")

    if args.limit:
        records = records[:args.limit]
        print(f"í…ŒìŠ¤íŠ¸ ëŒ€ìƒ: {args.limit}ê±´ìœ¼ë¡œ ì œí•œ")

    # â”€â”€ ëª¨ë¸ ë¡œë“œ â”€â”€
    client = None
    if not args.dry_run:
        if use_local:
            model, tok = load_local_model(args.local)
            args._local_model = model
            args._local_tokenizer = tok
            args.model_name = args.local
        else:
            from openai import OpenAI
            client = OpenAI(api_key=api_key)

    # â”€â”€ ì„¤ì • ì¶œë ¥ â”€â”€
    mode_str = f"ë¡œì»¬ ({args.local})" if use_local else f"OpenAI ({args.model_name})"
    print(f"\nâ”€â”€ ì„¤ì • â”€â”€")
    print(f"  ëª¨ë“œ:        {mode_str}")
    print(f"  ëª¨ë¸:        {args.model_name}")
    print(f"  ì¬ìƒì„± íšŸìˆ˜:  {args.regen_number}")
    print(f"  ì„ê³„ê°’:      {args.threshold}")
    print(f"  ë¶„í•  ë¹„ìœ¨:   {args.truncate_ratio}")
    print(f"  ìµœëŒ€ ë‹¨ì–´:   {args.max_words}")
    print(f"  Dry-run:     {args.dry_run}")
    print(f"  í•„ë“œ: abstract_hwt(Human), abstract_only_llm(AI), abstract_rag(AI)")

    # â”€â”€ ë°°ì¹˜ ì‹¤í–‰ â”€â”€
    start_time = time.time()
    results = run_batch(records, client, args)
    elapsed = time.time() - start_time

    # â”€â”€ ìš”ì•½ ì¶œë ¥ â”€â”€
    print_summary(results)
    print(f"\nì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")

    # â”€â”€ ê²°ê³¼ ì €ì¥ â”€â”€
    if args.output:
        output_path = args.output
    else:
        os.makedirs('results', exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_path = f'results/detection_{timestamp}.jsonl'

    save_results(results, output_path)


if __name__ == '__main__':
    main()
